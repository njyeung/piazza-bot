===== CHUNK 0 =====
Start Time: 00:00:00,000
Sentences: 3
Token Count: 82

--- TEXT ---
I'm happy to have you here today. As I'm sure you're all aware, there's going to be an exam this evening. And so be sure to be there, unless you've worked out an alt exam or something like that, or have some other arrangement that you've made with us. Let us know right away if you're sick and to not attend.


===== CHUNK 1 =====
Start Time: 00:00:17,670
Sentences: 6
Token Count: 160

--- TEXT ---
So I'm going to talk about storage resources more today. We were talking about file systems last time. I want to show a little bit about file systems and how they actually look when I'm logged into my Linux VM. And then we're going to be talking about, well, what do we actually put in the files in those file systems? And it turns out with the same data, there's often different ways to lay out or represent that same data. And that data representation has a lot of impacts for performance. And choosing the right file formats is one of the most important things you can do if you want to make your applications go faster. So first, I'm going to head over here to my virtual machine. And I'm already SSHN.


===== CHUNK 2 =====
Start Time: 00:00:52,270
Sentences: 7
Token Count: 292

--- TEXT ---
And, you know, I have some directory where I have some examples I may be doing later. But one of the things I want to do is I want to try to understand what this unified file system tree looks like. Remember that last time we would do these mount commands and we would stitch together a tree from all these parts, all these file systems that are on perhaps different partitions of different block devices. And so what does this unified file system tree look like? Well, we can tell if we run the mount command. The mount command is going to show us all the different file systems that are part of that tree. And over my lifetime, these have been growing more and more. 27 different file systems that are mounted as part of this big tree, some of which I'm very familiar with, some of which I'm like, huh, what is that, right? So I'm just trying to look through here and try to understand what some of these are doing. This is one that I recognize, right? So there's a proc file system, that's for process, and it's mounted at slash proc, and then there's some other details, like is it readable, writable? And so for example, I could go to slash proc, and I could see, well, here are some files here, and there's different files for different of directories.


===== CHUNK 3 =====
Start Time: 00:01:55,130
Sentences: 4
Token Count: 158

--- TEXT ---
What else do I have? The one I'm really usually very interested in finding is the root file system. And I can see that right here, right? So this one is mounted at the root, right? So this is really kind of like my starting file system. If I look over here, that would be like the blue one that I start with. And then all of these other things are mounted on directories underneath there, or maybe directories of other ones that are mounted on top of there. And so I can see, for example, this is an ext4 file system. That's something good to know, like what is the the main file system that your system is using. TempFS was that in-memory file system we talked about.


===== CHUNK 4 =====
Start Time: 00:02:30,630
Sentences: 10
Token Count: 227

--- TEXT ---
They mount it in different places. One is devshm for shared memory. A lot of applications will be using that. Lots of other file systems. I'm trying to think if there's anything else I wanted to highlight right now. I think those are the ones that I'm going to do right now. But you can see. You can try to dig through and see what all these different file systems are. If you want to, you can also run df. And df is going to tell you how much space you're using on the different file systems, you see this is a much shorter list because a lot of those were pseudo file systems, right? Like the proc file system doesn't really have any space associated with it. So these are the ones that are actually using space, either on a block device or on memory. And I can see what percentage is used, where it is at. If I want a human readable version of it, excuse me, these numbers like available and used are going to be a little bit nicer. So I might do that.


===== CHUNK 5 =====
Start Time: 00:03:19,570
Sentences: 7
Token Count: 441

--- TEXT ---
Another thing I do is I might say, show me just for the file system that is backing the current directory I'm in. And so I'm under slash proc right now. If I go back to my home directory, I could see this, right? So for my home directory, that I can see is on the root file system. And I've used 11 of 33 gigabytes. If I'm on the proc file system, it doesn't really tell me much because it doesn't make sense. There's no block device there. This is something you're going to have to watch as you do more projects, because especially with Docker, you'll probably be using up a lot of your storage space and you have to say well how much do i have and figure out how to go go clean that stuff up let's see how we can use the proc file system um to uh you know look at a process and see what's going on with it and so the the process i'm going to run is a sleep process and it's trying to sleep for i don't know 200 seconds but i might make it async so remember that i could do ps ax to list all the running processes and i could i guess i already see it but I could grep for sleep. Here's a sleep process. The grep process actually found itself, right? Grep found, you know, because it has that word sleep in it. Anyway, I can see it says process ID. So if I go to slash process and go here, there should be a directory with that PID number. I can see there's a bunch of things here. For example, maybe I want to see what was the command line arguments that were used to start that. It kind of looks weird because I don't see the space between sleep and 2000. that's because they aren't using a space character, they're using a byte containing a zero, right? But if I had a program reading that, I could easily separate that the argument's for sleep and 200.


===== CHUNK 6 =====
Start Time: 00:04:56,370
Sentences: 1
Token Count: 4

--- TEXT ---
Andy, yeah?


===== CHUNK 7 =====
Start Time: 00:04:59,290
Sentences: 1
Token Count: 8

--- TEXT ---
The microphone isn't good today.


===== CHUNK 8 =====
Start Time: 00:05:01,490
Sentences: 1
Token Count: 13

--- TEXT ---
Let me, you just normally give the other one a try.


===== CHUNK 9 =====
Start Time: 00:05:28,750
Sentences: 2
Token Count: 10

--- TEXT ---
How is audio now? Can people hear me?


===== CHUNK 10 =====
Start Time: 00:05:30,910
Sentences: 2
Token Count: 8

--- TEXT ---
Is that better? Okay, fantastic.


===== CHUNK 11 =====
Start Time: 00:05:32,570
Sentences: 3
Token Count: 79

--- TEXT ---
Yeah, any questions about this unified file system tree or some of the file systems that we're seeing that are mounted as part of that tree. All right, fantastic. So that's one thing I wanted to show you. I also want to show you how all of this relates to Docker. So I'm just going to go back and review some slides from last time.


===== CHUNK 12 =====
Start Time: 00:05:52,170
Sentences: 5
Token Count: 400

--- TEXT ---
That file system tree, unified file system tree, I have one of those when I log into my Linux VM. Each of my containers also has that. And there might even be some overlap between the one that I have in a container and the one that I have on my machine. that can let me do things like share files between a container and outside. So I'm going to go back to my demos about storage, and I'm going to take a quick look at this Docker file. This Docker file is installing a bunch of stuff. One of the things it's installing is JupyterLab, and so one of the things I want to show you is how when I start a container like this, I want to do the port forwarding like we've done before but I also want to map a directory on my host into the container right so we kind of forward along ports and we also have sharing files between that so when I'm working that container if I save my notebook you know I can see it on my VM and it's not going to get deleted as soon as my container dies and so I've already gone ahead and I've built this prior and what I'm going to do is I'm going to say docker run and I'm going to say it for interactive and I'm going to run this demo so it's basically just like an Ubuntu container and so I could do that right and what I want to additionally do is I want to do a volume mount and this is a lot like p right when we did p we would have we would have like the vm port and then to the container port. When I'm doing instead a dash v, then it is going to be the vm directory to a container directory. They often use the word volume to describe these directories that are mounted in. That's why it's a dash v.


===== CHUNK 13 =====
Start Time: 00:07:34,670
Sentences: 5
Token Count: 293

--- TEXT ---
And so what I could do is I could have some directory here. Maybe I'm going to say dot slash shared. And then I can say colon, where is it is under, maybe I'll, you know, let's call it, yeah, just inside. So I'm going to go ahead and run that. And then what I can do is I can say mount inside of here. And this is somewhat of a different list. One of the things I see first is that the root file system is an overlay file system. We talked about overlay file systems before. Every, the root of all these containers is going to be one of these overlay file systems. And that file system, instead of having its own block device is storing all of its data in a different file system in this case it's an ext4 on my vm and and that's great because i don't have to like buy like a new hard drive every time i want to create a container right this i can do a new file system but it doesn't have a new block device for it right it's going to have its own process file system this is a different one i would see different processes if i was there um and um and what's really interesting is that i have let me see what I ran it with again, I have that inside. So I'm going to go to slash inside.


===== CHUNK 14 =====
Start Time: 00:08:40,550
Sentences: 2
Token Count: 197

--- TEXT ---
And maybe actually if I go back to the mount, I'm going to just grab for inside and see what that is. That's ext4, which makes sense because I had an ext4 file system outside and I was mounting an ext4 directory inside here. And what's cool is I can touch files here. So if I say like touch hello dot maybe actually just like echo hello to message dot txt if i do that and then i exit my container then uh it was under this share directory and now i'm on my vm and i can see that right so stuff i'm creating uh inside i can see outside and vice versa all right i just want to show how we can also start a jupyter notebook with um with this sharing so i'm going to go back and run this again actually before i run it again i need to look at that docker file So inside, I'm listening on port 300.


===== CHUNK 15 =====
Start Time: 00:09:31,130
Sentences: 4
Token Count: 92

--- TEXT ---
So I'm going to go back to this. And so in addition to mapping in that directory, I'm going to forward along some ports. I'm going to maybe forward, you know, it could be the same. Why not? Or I'm going to forward 300 outside to 300 inside. You may have that directory there. If I want to access this from my, actually, I don't want to quite do that.


===== CHUNK 16 =====
Start Time: 00:09:52,110
Sentences: 8
Token Count: 186

--- TEXT ---
Inside of that Docker file, I have that default command, right? And so when I was running this, I guess I was saying override the default command with bash. I don't want to do that. I actually want that default command. And I think also I don't really want to be typing this. I want just Jupyter to run. And so I want to run it in detach mode. So I'm going to do that. I can see Docker PS. That's going to show me, okay, that thing is running. And now I want to try to access it, right? So I'm going to come over here and set up an SSH tunnel. I'm going to hit Control-R to search back dash L, right? Because that's how, you know, the dash L is how I do the port forwarding.


===== CHUNK 17 =====
Start Time: 00:10:24,410
Sentences: 5
Token Count: 440

--- TEXT ---
and I'm forwarding the 300 on my VM and then on my laptop it can be whatever port let's say 3001 it doesn't matter right and then I can come over here and I can say localhost 3001 all right so so what what am I looking at right now I'm looking at a Jupiter I'm connected to a Jupiter that's inside the container and inside of the container I have that special directory mounted which is called I may come back here and check again what was it called it was called inside right so I should have an inside directory over here which I do I have that message again which is great and what I would really do that is I do any kind of work inside of here maybe I'll rename this maybe like my p4 and I might you know I can do whatever I want here I'm gonna save this and then maybe when I'm over here, I can kill that container, right? So I'm going to say docker remove force competent faraday. And what's great is I still have under that shared, I have my work there. And if I were to restart the container again, right, if I go back and do a run again on that same directory, then I have a brand new container, a brand new Jupyter. All the other files here on the system are basically wiped, but I still have my homework here. And so it's very important because sometimes it's very sad, but I see people, they will, you know, do a bunch of project work and they do it in a container and it's not one of these shared things and then they delete their container and they lose a bunch of work. So I hope that doesn't happen to you. Anyway, that's why we go over it kind of carefully in class. Any questions about that example or how not to lose work? And of course, while I'm a bit of an aside, while we're on the topic of not losing work, what you should really be doing is you should test early on that this works, right?


===== CHUNK 18 =====
Start Time: 00:12:15,710
Sentences: 4
Token Count: 163

--- TEXT ---
Like I can see it on my VM that it's there. And you should push early and often to your Git repository to make sure there's no issues, right? Because sometimes I just delete my stuff. If I have it in a Git repository, well, that's fine. I'll just go back to the most recent version. I like to think about like if you're like in a Word document or something, how often are you hitting like control save? I'm kind of doing that a little bit obsessively. And kind of like Git push is like, well, that's what your commit and push is what you're doing instead of saving. So kind of do do it as often as you would in other scenarios.


===== CHUNK 19 =====
Start Time: 00:12:44,930
Sentences: 6
Token Count: 455

--- TEXT ---
All right, any last questions about file systems before we talk about file formats? All right, cool. I may head over here, so we're done with these slides, and we're gonna talk about what actually goes in these files, and I have a few different goals for you. One is that I want you to have some vocabulary to talk about different file formats, and some things I want you to talk about for a file format is what is its orientation, what encoding does it use, does it have compression, if so, what kind? And are there schemas built into this format? So I think everybody here is familiar with CSV files. So we're going to be talking about a new format, which is called Parquet. Parquet has a lot of advantages over CSV. If I'm working with any non -trivial amount of data, one of the first things I'll do is I'll just take my CSV, convert it to Parquet, and I'll use Parquet kind of going forward. That makes a lot of things easier and faster. When we're using these files, the performance is going to really matter on how we're accessing them, for example are we accessing them like sequentially from beginning to end or doing something else with it and so we're going to talk about these different workloads and two ways we're going to describe these workloads is as transaction processing and analytics processing right so we're going to learn what those are and then finally we're going to see that sometimes different storage systems are built for different kinds of workloads like this one is better for workload a this one's better for workload b and of course we want the best of all worlds so we're going to sometimes see how we can have different storage systems and we can extract data from one system transform it and load it into a different system that's called ETL. Data engineers do a lot of this ETL work, extract, transform, load. So we want to know kind of like what is that and why might we do it? And then we'll eventually have some projects like that as well.


===== CHUNK 20 =====
Start Time: 00:14:22,930
Sentences: 2
Token Count: 195

--- TEXT ---
Now when you're building an application, you can decide where you want to store your data. And one option is you could store your data directly in a file, right? For example, on the left, I'm storing some data in a file system and maybe I'm storing that in table.csv or some other format. And that file system is of course saving that data in blocks in a block device or maybe another file system. The alternative that we're going to be talking about more in upcoming lectures is that you could save your data in a database, but even then you have to think a little bit about the file system because that database usually doesn't use a block device directly. Usually the database is going to be storing its data in some files in a file system. Like maybe, for example, if I have a SQLite database, it's going to store that usually in a .db file would be the convention.


===== CHUNK 21 =====
Start Time: 00:15:04,130
Sentences: 3
Token Count: 114

--- TEXT ---
And so I just want you to be aware that, hey, there's these different approaches, and then we're going to be talking more about, well, when you might choose one or the other or whatever, the trade-offs here. All right, so when we talk about formats, there's these four things I want to talk about, orientation, encoding, compression, schemas, and I'm going to compare CSVs to these parquet files because they're different along all of these dimensions. So first off, orientation.


===== CHUNK 22 =====
Start Time: 00:15:28,430
Sentences: 2
Token Count: 155

--- TEXT ---
CSV is what we call a role-oriented format, and parquet is a column-oriented format. I want to look at what that what that means. We talked last time about this txt file that we wanted to access with all these weather stations around the world and this is not exactly a CSV file but it's kind of similar to that and what we saw last time is that if I think about where this data these bytes of data actually live in blocks this is across three different blocks and if I wanted to if I wanted to read all the stations names I'm not reading all the data but I still have to to read all the blocks in the end, right? Because the data that I'm interested in spans all of the blocks.


===== CHUNK 23 =====
Start Time: 00:16:08,210
Sentences: 7
Token Count: 334

--- TEXT ---
That'd be kind of slow. It would be good, though, if I just wanted to read a single row, because I know the offset where each of these rows start. I could go directly to it. If I want to read one row, I have to read a little bit extra. I have to read the whole block, but I don't have to read everything, right? So we're seeing there's different ways you can access this file. I could try to access it to get a whole column or to get a whole row. and depending on how we lay out rows and columns in actual files is going to make a big difference on whether this is fast or slow. All right, so there's some words for these and these, like so many of the words I'm introducing this semester, these words mean different things in different contexts, but the way we're using it today, transaction processing means that you're really kind of operating at the row level, right? Maybe I update a row, insert a row, delete a row, that kind of thing, select a few rows at a time, whereas analytics processing is It usually means that we're looking at specific columns, but we're looking across all the rows. And hopefully there's some intuition behind that. If I think about analytics, what are some of the simplest things you can do with analytics? You might say, what is the average value in some column? If I say, what is the average value in some column, well, I am going to be looking over all the rows but in a specific column.


===== CHUNK 24 =====
Start Time: 00:17:23,070
Sentences: 3
Token Count: 141

--- TEXT ---
Let's see how that actually looks. So here I have a table with some columns and rows. And it's two-dimensional. but when I save it like when I put it in memory or what we're talking about now is when I put it in a file we have to lay it out along one dimension right like you know each byte has some index and that's where it is and so there's two different file formats I could do right I could have a row oriented file where basically every row is consecutive within the file right so 15a would be 15a 26b would be 26b the column oriented format is where I put the columns consecutive within within that file.


===== CHUNK 25 =====
Start Time: 00:18:00,730
Sentences: 6
Token Count: 226

--- TEXT ---
So which is better? It depends. It depends on how I'm going to use this file. If I want to do transactions processing, that means I am like editing rows or looking up rows, and so for transaction processing the row-oriented format is going to be better because when I access that row it's going to probably fit within a single block or maybe a few consecutive blocks, right? But the column-oriented format would be terrible for transactions processing, right? Because, you know, if I have a row, right, each field within that row is in a different part of that file, right? So if I wanted to look up the 1.5a row and I have a column-oriented file, I'd be looking across all these different columns to get each piece, right? I'd actually have to and touch a lot of different blocks. All right, so in this case, transaction processing, row-oriented is the winner, and people often do that. What if I want to do analytics processing? I actually want to maybe add up all the numbers in column one.


===== CHUNK 26 =====
Start Time: 00:19:02,110
Sentences: 1
Token Count: 7

--- TEXT ---
Then the trade-offs flip.


===== CHUNK 27 =====
Start Time: 00:19:02,110
Sentences: 5
Token Count: 152

--- TEXT ---
If I want to do analytics processing, I really want a column-oriented format because all the numbers I care about are next to each other. They're compact and relatively few blocks. The row-oriented format is terrible, because I have to read in all the rows and then just pull out those small pieces of information that I'm interested in. And so what are real examples of these? Well, the CSV format is an example of that row-oriented file layout, and a parquet is a column -oriented. So if you're doing analytics and trying to answer questions about data, then a parquet is almost always going to be better than a CSV for that. Any questions about file orientation? All right. Cool.


===== CHUNK 28 =====
Start Time: 00:19:46,300
Sentences: 2
Token Count: 115

--- TEXT ---
Let's talk about encoding, right? So I have some data, but when it actually becomes bets and bytes, how does that look? And so I just want to kind of explain this by example. I want to explain a text encoding versus a binary encoding. So I have a large number here, and I could store it in different ways, right? I could store it as a string, right? That's what we do in CSV files, right, or JSON files. We take a number and we just put a string in the file.


===== CHUNK 29 =====
Start Time: 00:20:13,870
Sentences: 3
Token Count: 207

--- TEXT ---
Or I could use different parquet types, right? And when you're using parquet, you specify what type you're using. So I could say I can n32 or n64. And then depending on that, it would determine how many bytes I'm using, right? So the string would be using 10 bytes, parquet 8 or 4, depending on the size, right? So there's a potential to have a more compact encoding with parquet. You might ask, what if I have a smaller number like 12? Well, in that case, the string encoding, right, would be two bytes, which would actually be smaller than these parquet formats. But why is that? It's because the parquet format, they're trying to build something that is a format when they load it, that a CPU can quickly process the data, right? And so like if I have an n32 or an n64, there's CPU instructions for operating on that, right?


===== CHUNK 30 =====
Start Time: 00:21:00,650
Sentences: 4
Token Count: 387

--- TEXT ---
So the parquet is kind of more efficient for that. So even though Parquet, you know, is maybe using more bytes in this case, it has some advantages. If I really want to just use the fewest bytes possible, then there's a binary format that's still going to be the winner. That winner is protocol buffers. Protocol buffers, they're not trying to have a format that's like easy to operate on with a CPU. They're trying to have the format that's really kind of crunched down into the fewest bytes. And so in this case, if I had the number 12 and I put that in a protocol buffer on N32 or N64, that would actually only eat up one byte of data right so what we're seeing is that these binary formats can be more either more efficient in terms of you know it's good for the CPU to operate on or they might be more efficient in terms of using less bytes people often use text formats anyway because it's easier for developers right if I give you a CSV I can just open it with a program and and look at it whereas a parquet or protocol buffers lots of these bits and I need a special program to figure out what's inside but from a performance perspective binary formats are almost always better. All right, let's talk about compression and CSV files. I guess you could, you know, take a CSV file and then compress it. You can put in a zip file or something like that. There's no compression built into the format. Parquet files have compression built in. Lots of different compression algorithms out there. The one it uses by default is called Snappy. The idea of compression is that you want to basically use fewer bytes to communicate something, and the way you use fewer bytes is you avoid repeating yourself, right?


===== CHUNK 31 =====
Start Time: 00:22:32,150
Sentences: 4
Token Count: 308

--- TEXT ---
If I say the same thing repeatedly, maybe I can just reference back to what I previously said instead of saying it again. So for example, let's say I have a data file that has a bunch of street addresses and in Madison and they're separate by the semicolon. So it's like 1210 West Dayton Street, Madison, Wisconsin, 1202 West Johnson Street. That's also in Madison, Wisconsin. And so you can imagine if I had all these addresses there'd be a lot of redundancy between them, lots of addresses on the same street. Or even if there's different streets I'm using words like street and road frequently. And so the idea is if I wanted to shorten this text, I could have a reference here that's not the actual text, but I could just point back and say, oh look previously and what I said previously in this range of bytes is what I want to say again here, right? Now if you give a compression algorithm some file that has these redundancies in it, it's actually kind of computationally intensive to search through it and find places where it can find these redundancies. And really, the more work the algorithm does to find redundancies, the longer it's going to take to compress. So there's this space-time trade-off, right? If you kind of spend more CPU time finding redundancies, then you can end up with a smaller file.


===== CHUNK 32 =====
Start Time: 00:23:42,590
Sentences: 7
Token Count: 384

--- TEXT ---
And that's something some of you might all do, right? You might go out and you might benchmark different formats and say for a specific use case, I think we should use Snappy compression or something else. So what is Snappy trying to do? Snappy is kind of optimized more for speed, right? They want to very quickly compress stuff, but it's not going to give you the smallest file sizes. There's other compression algorithms that will spend more time to get you a smaller file in the end. What do I care about? Well, it kind of depends, right? If I like compress this file once and it's like some archive that sits there for like 10 years, then yeah, spending a little bit of time up front to have fewer bytes is good. If I'm kind of like regenerating a new file every day, right, it's trying to get deleted anyway, then I care more about being able to compress quickly, right? So I have to figure out for your use case what kind of compression format is going to make sense. Now, so compression is great, right? We can have fewer bytes, but there's also some problems with it. One of the reasons people don't compress things is that once I have this big file that's compressed, it's very hard to change it without rewriting the whole file. So for example, let's say I wanted to change this first address in the file. I can't just do that because other parts of the file are referencing back to it, and those other parts might not be supposed to have a change, right? So generally when you compress something, that's not kind of incrementally editable. So we'll often use compression for and then it's just done.


===== CHUNK 33 =====
Start Time: 00:25:12,870
Sentences: 1
Token Count: 3

--- TEXT ---
All right.


===== CHUNK 34 =====
Start Time: 00:25:14,670
Sentences: 3
Token Count: 197

--- TEXT ---
Another consideration about compression is that as it's looking for these redundancies, if I have a giant one gigabyte file, it would take a lot of time to look over the entire file for different possible redundancies. And so what they'll often do is they'll have windows. They'll say, OK, within this window, is there any redundancy? And OK, they'll figure that out. Then they'll go to another window, which is a range of bytes. They'll say, is there any redundancy here? And so that's what Snappy does. Snappy has a 32 kilobyte window, right? If I, you know, had redundancies that were like 100 kilobytes apart, it's not going to be able to find, see that. But if it's in that, within that window, it would be able to do it.


===== CHUNK 35 =====
Start Time: 00:25:51,790
Sentences: 1
Token Count: 8

--- TEXT ---
And so this also has some implications.


===== CHUNK 36 =====
Start Time: 00:25:54,810
Sentences: 1
Token Count: 88

--- TEXT ---
It kind of interacts with the row-oriented versus column-oriented format, right? So I have an example here. I have the row-oriented file, the column-oriented file, and I'm curious what you all think about which of these is going to be better at being compressed. So any volunteers, what do you think we can usually compress better, a role-oriented file or a column-oriented file?


===== CHUNK 37 =====
Start Time: 00:26:21,010
Sentences: 1
Token Count: 4

--- TEXT ---
Yeah right here.


===== CHUNK 38 =====
Start Time: 00:26:23,730
Sentences: 4
Token Count: 219

--- TEXT ---
You're more likely to have repeats within columns. Excellent, a column-oriented file is probably better. We're more likely to have repeats there, right? So for example if I have like a name column and an address column, right, maybe a lot of the same names might show off or like a lot of the addresses might show up again, right, but there's probably not as much redundancy between like a person's name and a street address. There'll be some, right, but not as much, right, so in most cases, right, you can think of some weird exceptions. If you have a column-oriented format and we put this compression on top of it, we're going to find more similar matches because while it's the same data type for one, it's all next to each other. And so this plays very nicely with with parquet, right, because it's tile-oriented, plus we're doing compression, all of this kind of nicely together. Any questions people have about compression?


===== CHUNK 39 =====
Start Time: 00:27:12,250
Sentences: 1
Token Count: 3

--- TEXT ---
All right.


===== CHUNK 40 =====
Start Time: 00:27:13,970
Sentences: 6
Token Count: 449

--- TEXT ---
So what I want to talk about now is schemas, and when I have some file and I'm using it, I need to know what types are involved, and so we're gonna have these schemas that tell us that, but CSVs are a little bit different than parquets. In a CSV, you have to infer what the schema is. With parquet, it's very explicit. What exactly is this schema? I kind of like this definition. It's a description of the structure of some data. And that description includes what the fields are and the types of those fields. And CSVs don't have that, right? It's just a bunch of strings. And you have to kind of guess, like, huh, there's a string that says 3 and another string that says 4, 5. Those all look like integers, right? And so when you're reading a CSV file, there's kind of two options. Like, let's say I'm using a pandas. I could say pandas read csv and then I have a csv. I could actually put in a dictionary where I tell it exactly like hey like this column is this type and and that's annoying right if I have like 100 columns that code would get ugly fast but that would probably be the fastest thing or I could just say you know I could say dtype equals none or just leave that blank and then we'll automatically try to guess what the type is and sometimes it does get it wrong right if it's the wrong type but even if it gets it right it's going to be kind of a slow expensive process to figure out hey, all these strings look like floats, or no, all these strings look like integers, or no, they all look like booleans, right? That's a slow, expensive process. And so what Parquet files do is this schema information, they put it directly into the file itself. There's no guessing, and you don't have to specify it when you read it. So you're going to be faster and more accurate.


===== CHUNK 41 =====
Start Time: 00:28:47,000
Sentences: 4
Token Count: 68

--- TEXT ---
All right, so I'm going to head over here, back to the notebook we were in, and I'm ready to do some demos. Let me create a new file. I'm going to call this demo. All right. Let me try to get some more real estate on this screen. All right, great.


===== CHUNK 42 =====
Start Time: 00:29:09,620
Sentences: 3
Token Count: 104

--- TEXT ---
And so what I want to do is I want to look at with PyArrow. PyArrow is going to let me read CSV files. It's going to let me read Parquet files. And so I want to do some examples with PyArrow. So I'm going to import pyarrow as pa. I also want to import pyarrow.csv, and I want to import pyarrow .parquet, even though I'm importing it like this.


===== CHUNK 43 =====
Start Time: 00:29:34,590
Sentences: 1
Token Count: 6

--- TEXT ---
That's not right.


===== CHUNK 44 =====
Start Time: 00:29:37,840
Sentences: 2
Token Count: 413

--- TEXT ---
All right, so I'm going to do those. Even though I'm importing it like this, that will allow me to say like p.csv and then do various things with that. Now, I'm not trying to show it again now, but in that Docker file, I already downloaded this big csv file over here which is in my root directory right so i have this um hdma uh wisconsin these are real loans right so maybe i just copy the name of that i can use that for some of my examples so that is the file i'm gonna be working with and i want to show well you know how like how long do things take when i'm accessing it from a csv or a parquet Okay, and so what I may first do is I'm going to say pa.csv .readcsv, and I am going to just read it like that, and I'm going to get that into a pie arrow table, so that'll take a while, and I just want to point out that this is, in my example, this is going to be like the slowest thing I'm doing here, but I want you to remember in a previous demo, I was comparing pi arrow read csv to pandas csv and this was a lot faster than pandas csv right so kind of the the slowest example i'm giving today was like the fast example uh compared to pandas which is our previous baseline and that was because even though this has to do schema inference right like it did schema inference right now and it says uh state code is a string uh you know this one's an 64. Even though it's doing that, it has that kind of cache-friendly layout for strings. I was able to do that schema inference faster than pandas can. But again, right, it has to do schema inferences. This is going to be a lot slower than not doing schema inference at all.


===== CHUNK 45 =====
Start Time: 00:31:13,350
Sentences: 6
Token Count: 325

--- TEXT ---
All right, so I have that table. And I can write it. So I can say pyarrow .parke.write table. And I'm going to just write that to, I'm going to give it the same name. I'm going to let it be in this directory. and I'm going to call it parquet, right, so I'm going to do that, and that's not good, so it says where, let me just take a quick peek on that, oh, you know what it is, the first argument is the table, the second one is the path, so I'm going to write that table, and then I can probably see that, I guess I'm still on the inside, right, so I can see that I was able to write it over here, and then what I could do is I could read it back, and if I wanted to, right, and so this is what I'll often do these first two lines. I will often, now it's a parquet file, right? Maybe it's called read table, right? Great. Oh, and then, sorry, I'm trying to really struggling with my paths today. But this is what I'll often do. I'll have two lines like this. If somebody gives me a CSV file, I'll just like, you know, read it in, do the schema inference once, write into a parquet file, and never do schema inference again and always reuse that same file.


===== CHUNK 46 =====
Start Time: 00:32:21,910
Sentences: 11
Token Count: 388

--- TEXT ---
And let me show you what the performance looks like at. So this is a magic line, this percent percent time. It's not Python code. It's a Jupyter thing. And when I run that, it's going to show me how long something took. And wall clock time means I looked at the clock on the wall, and I saw what time it was. I did something. I checked again, and I got the difference. And so I have that. And then I'm going to do this down here as well. When I read this one, this one is dramatically faster. what is that, like three or four times faster? So I see that that's a big improvement. In addition to wall clock time, I have the CPU time, and the CPU time was quite a bit more than the wall clock time. And so, for example, what does this probably mean? This is almost four times bigger. It means that PyArrow was using multiple threads to do this for me, right? They had to have multiple threads to use more CPU time than actually passed in the real world. It's about four times more, That makes sense to me as well, because I have four CPUs on this machine, right? So for about a half a second, it was using four CPUs for about four, you know, two seconds total of CPU time. So not only is using the parquet a lot faster, they're both parallel, but if I'm thinking about overall load on my machine, right, I'm using dramatically less CPU, right, like four times less CPU if I'm using the parquet format, right? All right, so my first point here, right? So point one is that parques don't need to waste time on schema inference. So that's a huge, huge benefit.


===== CHUNK 47 =====
Start Time: 00:33:57,970
Sentences: 8
Token Count: 271

--- TEXT ---
All right, let's talk about encoding now. Maybe, yeah, and so my point two will be that. Point two is that parques are a binary format. Okay, so how can I do that? Well, one way I could see that is I could just open it off. So I could open up this file here with open, and I could just open it for reading as a binary file as f, and then I could read some number of bytes, like maybe 100 bytes, and I could just print that off if I want. And so I see that it's this binary format. It's lots of good things about this format, but it's not just like I can look at it with any editor and see what's going on, right? with the csv though right if i open up my csv which i think was at my root right it's it's bytes right but it's bytes in a format that's that's human readable that's one of the reasons people use csvs even though it's they're so bad along all these other dimensions all right the next thing i want to talk about is column orientation right so i'm going to say this is point three. So I'm going to say parquet is column oriented.


===== CHUNK 48 =====
Start Time: 00:35:11,060
Sentences: 6
Token Count: 277

--- TEXT ---
So we can read only the columns we want. Okay, let me show you what that looks like. If I come back here to what I'm reading this here, there's an option to say I'm only interested in specific columns. So for example I can say columns equals maybe I want like the LEI column and maybe like what else a census tract column just trying to get a couple columns here maybe call this t2 let me see how long this took right now this is super fast right because I didn't have to read that whole file only grab the data I want to grab it so again I just want to think about the evolution of this right so So earlier in the semester, I showed you Pandas. Pandas is really slow at schema inference because they don't have any efficient layout. And then I'm like, oh, well, Pandas with CSV, if I want to be faster, I should switch to pi arrow with CSV. That's faster. If you want to be even faster, switch to parquet instead of the CSV. If you want to be even faster, then only read in the columns you want. So we went from 20 milliseconds to the fast case to, you know, like we've been even slower than this, right? So orders of magnitude faster.


===== CHUNK 49 =====
Start Time: 00:36:26,740
Sentences: 1
Token Count: 2

--- TEXT ---
Okay.


===== CHUNK 50 =====
Start Time: 00:36:27,740
Sentences: 1
Token Count: 16

--- TEXT ---
Very important if you have lots of data and you want to process it quickly.


===== CHUNK 51 =====
Start Time: 00:36:31,020
Sentences: 5
Token Count: 294

--- TEXT ---
Any questions so far? The last demo I wanted to do is about compression. And so when I have this data, right, I can write it out. dot write table t and um by default there's this compression option which is a snappy by default so i'm just trying to write this out to a snappy dot parquet um i'll do that as my first example let's see how long that takes right so i'm going to read um write my data to um a snappy parquet let me um do gzip instead gzip is going to spend a little bit more time to try to get me a better compression ratio and um and my point here right point four is that um is that parquet is compressed and you can uh trade off space time by choosing how right is the point i want to make here and so when i write to snappy this kind of matches what they said right they optimized to be able to compress very quickly right so 700 milliseconds or so i'm doing parquet it was about three times longer. What if I do ls-lh and then I'm going to do star.parquet? Well, I can see that the snappy.parquet was 16 megabytes. That was also the default, so that's what I saw before. And then the gzip parquet was only 13 megabytes.


===== CHUNK 52 =====
Start Time: 00:37:57,160
Sentences: 3
Token Count: 68

--- TEXT ---
Is it okay to spend three times longer to make it a little bit smaller? Maybe, maybe not. It depends on how long we're going to have this file before we'd have to compress again with something else. All right. Any questions about any of these trade-offs between CSV files and part K files.


===== CHUNK 53 =====
Start Time: 00:38:14,420
Sentences: 7
Token Count: 396

--- TEXT ---
Bless you. All right. Cool. So I think that was all my demos I had for that. Let me head back here. And so we've been learning about all these different file formats. Of course, that's important whether we're using files directly or if we're using a database, because a database is usually going to store its data in files as well. So I want to start talking about different databases that are out there and how they really fit in with all the resource management we've learned and about how they can be optimized for different things. So what is a database? Well, it's a collection of tables, right? Those tables have names, and they also have schemas, right? So a schema will tell me what the column names are and what the types are. There are often what we call keys that can relate different tables together. So for example, on the bottom right, I have this table loan, and then there's a state ID. And so I see that that's state number two. If I want to know what is state number two, I would go look up in the table state. I can see well state number two is Alaska, right? So they're going to give some names to these IDs that we're putting right in the table state. That lookup code I'm using, we call it a primary key in most cases. And in the table loan, that number that I used to do the lookup, I'm going to call that a foreign key, right? So where you have all these tables, they have columns, they have types on them, and there's relationships between the tables represented by these primary and foreign keys and often people will look at this and they'll say this is a relational database that's the relationship they're talking about is with the foreign and the primary keys.


===== CHUNK 54 =====
Start Time: 00:39:42,800
Sentences: 4
Token Count: 201

--- TEXT ---
So in addition to storing all your data for you you can also ask questions about your data kind of a one word for a question is a query and so you're going to be giving these databases queries you know I guess when I was looking at this here like I could imagine my questions in English like what is the name of the state with Wisconsin as an abbreviation or it's kind of a little bit strange. Like in normal usage, when I say like query, it means I'm asking a question, but we also, in the context of databases, a query might be directions to make changes to the data. So if something like insert portal recall as a state, if that were to happen, we'd have to update our data and that would be expressed as a query. Now, when we're interacting with the database, we don't use actual English, at least not usually. Maybe that's changing with all the LLM stuff that's happening.


===== CHUNK 55 =====
Start Time: 00:40:26,920
Sentences: 3
Token Count: 306

--- TEXT ---
Are you going to usually use some kind of structured way of formulating our queries, right? These queries are going to have different clauses. There's different query languages out there where I learned some of them in the semester, like where I learned the Cassandra query language, for example, but by far the most famous and well-known is SQL. SQL is a structured query language. It has these different clauses. It has like a select clause for asking questions, and then it has other clauses for changing the data like insert, update, and delete, right? So this is a skill that you all should build, right? We're going to do it quickly, it's not the main focus of the course, but you should all like, you know, get very comfortable writing SQL queries because so many systems support this language. All right, so when I'm looking at this here, there's different questions I could ask, different ways I could interact with the database. I could, for example, say select the average interest rate from the loan table, right? And that's an example of an analytics question, right? It's kind of going to be looking at an entire column of rates and doing that, right? So a SQL query could help me do analytics. A SQL query can also help me change the data. Maybe I'm selecting a specific loan application from the data, or maybe I'm inserting a specific row into the data.


===== CHUNK 56 =====
Start Time: 00:41:34,640
Sentences: 8
Token Count: 195

--- TEXT ---
That's transactions processing. And so one of the really cool things is that SQL as a language is very general, right? It's useful for both describing transaction processing work and analytics processing work, right? Now one of the things is where I see later is that even though SQL as a language is good for that, and a lot of databases will kind of support all of these queries, a database might be good at one kind of query, right? And so one of the skills I want you to walk away with is that you'll think like, hey, this is the application I'm building. These are the kind of queries I'm going to do. What kind of database is going to be a good match for the queries that I'm doing? And a lot of that will come down to the databases architecture and how it's built. So architecture, what is architecture? It means different things in different places.


===== CHUNK 57 =====
Start Time: 00:42:22,200
Sentences: 3
Token Count: 93

--- TEXT ---
But when I'm talking about software architecture, what I really want to know is what are the big pieces of it. I'm not looking at lines of code or function. I want to know what are the big pieces and how do they interact with each other. And so we'll often draw that. It's like different subsystems as part of a bigger system, how they interact. And there's lots of different database architectures.


===== CHUNK 58 =====
Start Time: 00:42:42,820
Sentences: 1
Token Count: 10

--- TEXT ---
I just grabbed this random one from a book.


===== CHUNK 59 =====
Start Time: 00:42:45,890
Sentences: 6
Token Count: 431

--- TEXT ---
and my point is not that I want you to learn this specific architecture, but I want to see about how all the different resources we learned this semester fit into this picture. We've learned about storage, memory, network, and compute. I just want to highlight that, right? So a database is going to be storing its data on disk with the help of a file system, and so there might be some access methods that are trying to figure out different file layouts, right? So we definitely have to think about storage when we're doing databases. What about memory databases often have some caching maybe an lru cache probably actually something more sophisticated uh they're going to call that the buffer manager but they're using some memory so they don't have to go to disk every time right so all the stuff we learn about memory well that's going to be part of a database um what about uh network um there are some databases that are embedded so the database is like in the same process that's using the databases but more often what we have is a server client, right? So like the database is somewhere, it's a server, I send queries to it over the network from a client, a database client, and so we have networking involved when we're using databases as well. And then finally we have compute, right? We have to do computation over the data that's either in our cache or maybe on disk, and we have to do that execution to actually answer questions about the user's data. They give us a query, we have to figure out what that query means and what computation we have to do to answer. So all these resources we've learned about, they're gonna fit in into this database picture. All right, so databases are doing a lot of things for us. And a lot of people, kind of their default mode is if they have any data, they're gonna put in a database. In a lot of cases that works out well, but we don't have to, right? We have to make a decision now.


===== CHUNK 60 =====
Start Time: 00:44:24,940
Sentences: 10
Token Count: 413

--- TEXT ---
When do we put data in a parquet file somewhere or when do we put it in like a full-fledged database? And I think that it really depends on how you're going to want to use this data. And this learning Sparkbook, I feel like, really described the trade-offs here nicely. So first off, what's one of the big advantages of putting in the database from a performance perspective? Whoever built that database, like the engineer's building it, they're deciding what the file layout is, and they're also writing the code that does the queries. And if you design systems together, you can often get certain efficiencies. Like, for example, maybe I'm, like, you know, trying to have, like, something that's doing some kind of query processing, and I'm, like, oh, we could run this specific kind of query faster if we had some data structure stored in our files, right? And so, okay, you'll say, let's do that. We're going to put in the file, then we're also going to change our code that's doing the execution, right? So this tight coupling or tight integration can be very efficient, right? And that's why people often like that. Now, what if I'm just putting my data, like, in a parquet file, right? Like, well, some people wrote Partey. Lots of people coordinated on that and kind of agreed what Partey is. And then I can't change that. Partey is kind of fixed, but I make my own application that's using these Partey files. You know, I can't tightly integrate in the same way, right? So why would I want to do that? The reason you don't necessarily want to tightly integrate is that there might be a lot of different ways that data is used, right? And you can only tightly integrate with one system.


===== CHUNK 61 =====
Start Time: 00:45:51,640
Sentences: 3
Token Count: 233

--- TEXT ---
So, for example, let's say I have this big database, and then I want to do machine learning on the data in it, right? And I want to use something like PyTorch or who knows what to do the machine learning on the data in the database. That data, the database in a very specific format that I can't directly use. So what will I probably do? I will probably first do a giant query, like select star from this database and pull everything out and put it in a separate file that I can use with PyTorch or whatever other machine learning thing. And so that kind of raises a question, right? Like, if I'm not just immediately going to pull all the data out of my database anyway, why do I want to closely integrate my database format with the database itself, right? So kind of instead of doing that, we'll kind of have these open, more generic formats like Parquet, and then lots of different things could use it. Maybe I have one tool that does queries over it. Maybe I have a different tool that does machine learning over it.


===== CHUNK 62 =====
Start Time: 00:46:44,300
Sentences: 1
Token Count: 3

--- TEXT ---
All right.


===== CHUNK 63 =====
Start Time: 00:46:46,240
Sentences: 10
Token Count: 396

--- TEXT ---
So I want to look back at this again and see how this plays out with these different kinds of workloads. So as I mentioned before, some queries that are going over an entire column are analytics processing. Other ones that are operating at this role level. Oh, I'm sorry. The first one is analytics processing. The things at the role level are transaction processing. And so SQL as a language works well for both, but it's hard for a database to be good at both because the database, when it's storing its data, has to figure out, am I a column-oriented database or a role-oriented database? And whenever they decide one way or the other, then there would be a fast at one kind of query or slow at the other kind of query. And so databases will often give themselves these names. They'll say, I'm an OLTP database, or I am an OLAP database. And I don't really love the acronyms because there's only one letter in them that is interesting or different, right? OL means online. People don't even necessarily know, well, why are we saying it's online, right? Like, what does that mean, right? People speculate. And then processing, well, of course, we're processing data. So OLTP, OLAP, I look, is it a T or an A? and it's either a transaction processing database or analytics processing, right? So I want you to be able to kind of think through this. Like if I say, like, hey, like this application is doing these kinds of queries, you should say, like, oh, this is an analytics query. And then you can read about different databases and you're like, oh, this is an OLAP database. You might say, oh, this OLAP database is a good fit for our needs.


===== CHUNK 64 =====
Start Time: 00:48:10,180
Sentences: 1
Token Count: 3

--- TEXT ---
All right.


===== CHUNK 65 =====
Start Time: 00:48:11,520
Sentences: 8
Token Count: 345

--- TEXT ---
So whether it's OLTP or OLAP is going to have a lot of impact for what these access methods are. now what if you need both and this is often the case right if you're a big organization there's gonna be lots of different databases again this book which I love has this example where they have all these different OLTP databases for different things it's an e-commerce site and you know there's one database backing the website something else keeps track of the inventory and the warehouse maybe like the truck drivers have some database and that's all great for operations but if you want a data scientist to come along and analyze this then it's not so great right because our data is spread across all these different databases, and it's all transaction processing. That's good for analytics. And so what we'll often do is we'll create a big column-oriented database, and we're going to call the data warehouse. That's an OLAP database, and we're going to say, hey, let's pull all the data from all these different sources and put it there, right? And so people will have to write all these jobs that we call the ETL jobs that extract data from different things, places, transform it, and then load it into that data warehouse. Once somebody does all that data engineering, that ETL work, This is going to be a great setup for the data scientists to come along and analyze that data. All right, so we'll break there. I have some time if people have questions afterwards. Otherwise, I hope you all have good luck on the exam tonight. Have a great evening.


