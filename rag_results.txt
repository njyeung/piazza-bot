=== RAG Query Results ===
Question: What is the difference between OLAP and OLTP? Which one should is column orientated and which one is row orientated?
Class: CS544 | Professor: Tyler | Semester: FALL25
Top 20 most relevant chunks:
================================================================================

[RANK #1] Chunk 11 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Michael%20Doescher-Morgridge%201570-10_08_25-09%3A53%3A58/1_vudu9ulk
Timestamp: 00:18:00,730
Token count: 380
--------------------------------------------------------------------------------
So which is better? It depends. It depends on how I'm going to use this file. If I want to do transactions processing, that means I am like editing rows or looking up rows, and so for transaction processing the row-oriented format is going to be better because when I access that row it's going to probably fit within a single block or maybe a few consecutive blocks, right? But the column-oriented format would be terrible for transactions processing, right? Because, you know, if I have a row, right, each field within that row is in a different part of that file, right? So if I wanted to look up the 1.5a row and I have a column-oriented file, I'd be looking across all these different columns to get each piece, right? I'd actually have to and touch a lot of different blocks. All right, so in this case, transaction processing, row-oriented is the winner, and people often do that. What if I want to do analytics processing? I actually want to maybe add up all the numbers in column one. Then the trade-offs flip. If I want to do analytics processing, I really want a column-oriented format because all the numbers I care about are next to each other. They're compact and relatively few blocks. The row-oriented format is terrible, because I have to read in all the rows and then just pull out those small pieces of information that I'm interested in. And so what are real examples of these? Well, the CSV format is an example of that row-oriented file layout, and a parquet is a column -oriented. So if you're doing analytics and trying to answer questions about data, then a parquet is almost always going to be better than a CSV for that. Any questions about file orientation?
================================================================================

[RANK #2] Chunk 25 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-10_10_25-09%3A55%3A19/1_dxhhhv7w
Timestamp: 00:41:54,560
Token count: 265
--------------------------------------------------------------------------------
What I'm going to do is I am going to read that in with Pyre. I'll write in that parquet file. I actually have lots and lots of different columns. I'm only going to read in the columns that I want to do, and because it's a column-oriented format, I don't have to read and decompress most of the file. So that will get me that file, and it's going to look like this. It'll be very quick. If I did not have these columns, it would be noticeably slower. And what I want to do is I want to take it and I want to put it into SQL, right? And so I think the easiest way to do that is to first convert that pyarrow table to pandas, sorry, to pandas, because a pandas data frame can be rented to SQL. So I'm going to write it to SQL, I'm going to write it to a loans table, where I use the loans table for examples. It's going to be on that database connection we have. What else? I'm just checking my notes here. I don't want an index column created. And if it exists, I'm going to say replace.
================================================================================

[RANK #3] Chunk 9 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-12_08_25-09%3A56%3A12/1_pjpztqno
Timestamp: 00:13:09,290
Token count: 371
--------------------------------------------------------------------------------
And the idea is that the blocks are basically sorted based on some criteria that I specify. So all the values in block one are going to be less than any of the values in block two, for example. Now, sorting is expensive, so they're not too strict. Within a block, there's no order to the rows. They are not sorted within a block, but the blocks overall are sorted. And then I keep statistics in on each column, right? for each column, what range of values are in there. And that's going to help me with a query, right? If I have some query and there's some where clause filter on it, I can look at my different blocks of my table and I can see, well, any rows that are going to go in this might be on blocks one and three, but not the other blocks, right? So I can do that. So it's semi-sorted. That helps me look at a subset of the data instead of looking at all of the data. Now, one thing I might worry about is that if I want to write some new data, you know, let's say I'm writing 10 new rows, chances are those 10 new rows are going to be spread across all my existing blocks. And what I don't want to do is a bunch of small random writes. Small random writes are bad for hard drives, right, because hard drives have to seek their head to do their writing. Small random writes are also bad for SSDs because SSDs have block erase, and then I may have to do garbage collection within the SSD. So small random writes are always bad. And if I'm not careful, if I have this clustering pattern, I'm going to have small random writes, okay?
================================================================================

[RANK #4] Chunk 28 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-12_01_25-09%3A55%3A50/1_928i3yai
Timestamp: 00:48:06,060
Token count: 340
--------------------------------------------------------------------------------
That's one option, right? So I have the tables, external tables, and then views which are in some ways similar to external tables, except they're referring not necessarily to an external source. Now, the other term that you might encounter is something called a materialized view. And the idea of a materialized view is I have some data source that's based on other things. But instead of rerunning the query each time, I just go ahead and instead of creating a view, I create a table and I actually compute those results and I store them somewhere. In this case, we would store them as a capacitor file. I could do that as well. And so I'll just end with this last thought. So which approach is going to give us a better performance when I'm using beach animals? Do I want, let's hold up one finger if you want the view, two fingers if you want the table, right? I just want to have better performance when I'm querying it. Which one's going to be better? I see just a couple people saying two, which is correct. I want to have the actual bytes of data so I don't have to refilter every single time. Which is lower crossed? Well the view is lower across, but I don't have to be storing multiple copies of my data. So we're going to come back next time and talk more about how we can manage all of this. But I think that's a good breaking point there. So have a fantastic day, and I'll be around if anybody wants to chat or has any questions.
================================================================================

[RANK #5] Chunk 2 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-11_05_25-09%3A55%3A10/1_qjba2w7v
Timestamp: 00:03:24,710
Token count: 341
--------------------------------------------------------------------------------
And all of those, what I want to do is I want to choose the one that results in the lowest impurity, right? What is impurity? If I put a bunch of rows together where there's a mix of RAID and no RAID, that would be high impurity. What I ultimately want to do is I want to ask questions where it will be like, oh, it almost always RAIDs or almost never RAIDs. That would be a higher purity, right? And so they really just brute force it. They consider all possible six questions and see which one reduces impurity the most, right? And there's different metrics for that. I'm going to leave it a little bit vague because I don't want to get into the math of it, but I just want to think about what things we'd have to measure to estimate whether there's high or low purity. What we would do is we could compute a table like this, and what this table is saying is for a given way we split the data, some rows will go left and some will go right, and on both sides where I have some yeses and nos on the left and some yeses and nos on the right. So for any possible split where I have these metrics like this, these four numbers, and again I'm not trying to like put a formula on the stream because that's not the point of this class, but the impurity metric is going to somehow be based over these four, and the purity metric will be lower if I'm more uniformly putting like all the yeses to one side and all the noes to the other side.
================================================================================

[RANK #6] Chunk 2 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-12_03_25-09%3A54%3A24/1_bbavo2sj
Timestamp: 00:03:17,690
Token count: 419
--------------------------------------------------------------------------------
I wouldn't have to do any kind of filtering or whatever else that query is doing. So there's often performance benefits to materializing that view and having like an actual table with that kind of downstream data rather than recomputing it on the fly every time. So that's desirable, but we have to worry about what happens when data changes upstream, right? And so I'm trying to imagine, for example, maybe there are some new rows appended to these parquet files over in my GCS bucket, and so I have updates there, and when those updates happen there, updates may or may not be visible at other points in the system. If I just have an external table that's referencing it, then of course I'm immediately going to, you know, if I'm using that external table, I'm immediately going to see those updates. But you can see down here I had a problem, right? This capacitor file, I had previously loaded beaches into it, and I had a copy of the data, and now that copy is old. I don't have the new information. And similarly, my materialized view over here doesn't have the latest information, right? So I might have to redo some work when data changes to recompute downstream data. And so could I just rerun that query that produced the beach animals materialized view? And the answer is no, I cannot do that because even though I would get some fresh data from this side, I'm recomputing beach animals based on this beaches data set that is stale, right? So if the beaches are stale, it's old data, then beach animals is going to be old data as well, right? So you can start to see that maybe this is actually a tricky problem, And if I want to, you know, save the results of some query in some table and I want to be fresh, then I have to think carefully about which queries I need to rerun when my data changes, right?
================================================================================

[RANK #7] Chunk 22 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-10_17_25-09%3A54%3A10/1_u6knxi3c
Timestamp: 00:39:23,610
Token count: 465
--------------------------------------------------------------------------------
You'd say, oh, there's a column of letters, A, B, A, B. There's a column of numbers, 1, 2, 3, 4. And I want to do some computation on it, and what I want to do is I want to have two times the number for all the A's, right? So I guess like I have A1 and A3, so I want to basically get two and six out, right? I want to do those transformations to get it. And the way I could do that is I could do a bunch of operations using a spark context. This sc is spark context. And this is actually a real code here. And what I could do is I could basically pass in different functions that are transforming the data in some way. With MapReduce, we would only pass in map and filter functions. With Spark, they're like, that feels a little restrictive. Let's add filter. They have other things as well, sort, lots of different things instead of just, you know, restricting you to map and reduce. And so I see there's a few here. There's parallelize, map, and filter. There it helped me transform my data in some way. The parallelize, all that's doing is it's taking my data from, you know, like regular Python data structures, and it's putting it into a spark. Map is, what it's doing is taking a function, right? And it's going to call that function on every single row, and that function is going to return a new row, right? So we put a row in, get a different row out, and so I can see this malt2 up here, it's taking one of these rows, for example, maybe it's taking this row a1, and we return a new tuple. The first part is still a, and the second part is two times the number, so instead of one, it would be two, right? So I'm saying table.map because I want to do that on all of my rows. The .filter, right, I also pass in a function for that, I'm passing in only the A.
================================================================================

[RANK #8] Chunk 5 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-10_10_25-09%3A55%3A19/1_dxhhhv7w
Timestamp: 00:08:12,980
Token count: 426
--------------------------------------------------------------------------------
We have these tables in them. We want to change the tables. And where we do that is part of something called a transaction. and you know kind of a theme of this course is that we use the same word to be different things in different cases and I apologize for that I didn't choose a lot of these words but transactions is one of those words because last time when we were talking about transactions we were talking about a workload like how do we access the data what pattern is there we talked about this analytics workload pattern where we look at a whole column at a time and a transaction processing pattern where we would kind of, you know, update or look up rows at a time, right? So that was one kind of meeting of transactions, and we would talk about OLTP databases. Transactions also can refer to a group of updates we're making together and some properties we have about them. And that's what I'm going to be talking about right now. I want to update a database, maybe make multiple changes, and what can I guarantee about those multiple changes? And this should kind of remind you of locking. When we were looking at critical sections, you can remember before we checked, hey, is the bank account greater than some dollar transfer amount? If so, then subtract it from my account, add it to the other. We probably want those things to appear as if they're happening at the same time, right? There might be other guarantees we want. Like, for example, if my computer crashes and I reboot it, I don't necessarily want it to forget that this transaction happened. And so there's all these guarantees we can have about changes like this. The four guarantees that are really kind of common that you should remember are atomicity, consistency, isolation, and durability. And that's actually a nice acronym. That's ACID, right? So people often talk about ACID databases and say, like, hey, ACID databases have these four things. So let's look at what each of these means.
================================================================================

[RANK #9] Chunk 7 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-11_07_25-09%3A54%3A47/1_sk2olosx
Timestamp: 00:11:50,010
Token count: 389
--------------------------------------------------------------------------------
In terms of things that do not repeat, a station ID would not repeat, right? I have like one station ID and a whole bunch of weather reports for it, and then also a station name, right? Now you can see the difference there, even though neither of those are repeating, the station ID is something I use to uniquely identify it. I can't have different partitions with the same station ID. The station name, it doesn't repeat, but it could really be whatever. I guess like if I wanted to, I could have two different stations with different IDs that have the same name. The database wouldn't stop me from doing that, right? The static column, all they really do is they say, hey, like we have like one static column associated with that partition key, which in this case is one, two, three. Now we also, in addition to that, things like weather data repeat, right? I keep getting more weather data over time, and so those would be over here. And we can see that I might want to look up the weather for a specific station on a specific day. And so to do that, that's a specific row of data, and we need to have some way of identifying that specific row of data, and that is the primary key, which will have this part of the station ID, and then I've made date into the cluster key, right? So the cluster t is what identifies a row within that partition, right? So if I give you the whole primary key, if I give you a station and a date, then you can find one of these rows over here, and you can look up all the regular cells or all the regular columns that are associated. Maybe like the temperature, you can imagine they can add other things like humidity or who knows what, right? Maybe mid or max temperature for the day.
================================================================================

[RANK #10] Chunk 12 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-10_27_25-09%3A54%3A06/1_bvt1onkv
Timestamp: 00:18:57,170
Token count: 368
--------------------------------------------------------------------------------
If you can remember the first three, you'll probably be able to remember the other three because that's just kind of these three with some variations right so three i want to talk about are memory only memory only serialized and disk only right so i can persist df2 with any of those right so i think the simplest one is memory only right what that means is that when i actually compute the data for these df2 partitions i'm going to put it in memory and keep it there in case i need it again later and that's equivalent to what we've already been doing right when we've been doing df.cache, for example, what that is doing is basically a persist with the memory-only level. So that's great. Now, I want to think a little bit about this data that we're persisting. Spark was written in Scala, and the way they're running the Scala is that the Scala compiles to JVM bytecode. So this is running on the JVM, and we have these JVM types, and it turns out like the Java and JVM types are notoriously bloated in terms of memory, right? And there's different reasons. One is that they have these object headers, and an object header is 16 bytes, right? Lots of stuff in there. Maybe there's like a point or two, you know, the class for a type. But you could imagine, right, if I have a four byte integer, but then I use the actual object type for that, you know, in Scala, well, all of a sudden, it's going to be 16 plus four. It's going be like 20 bytes instead of four, so five times overhead.
================================================================================

[RANK #11] Chunk 17 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-12_01_25-09%3A55%3A50/1_928i3yai
Timestamp: 00:27:17,810
Token count: 468
--------------------------------------------------------------------------------
It's this array of structs, and maybe I guess I could be like a line, maybe it's like from one point to another, maybe I can have it longer if I wanted to. And what I really want to do is I want to get every x, y, z combination in my output data. And so there's a few parts of this. The first part is that I'm going to unnest that column that has an array of structs. And when I do that, what happens internally is that I'm going to basically get like a little table for each of these. I have a different logical table corresponding to each of these rows where I'm stacking up all those structs like a table. And then what we're going to do is we're going to look at the simple values from my original table and we're going to cross -join that with these little kind of virtual tables that that I expanded and we're going to look at every combination. Well at least we would look at every combination if it were just a simple cross-join but this in this case when I'm doing an unnest, the kind of cross-join it does is what is called a correlated cross-join. And what that means is that it's going to pair up x values with things that are unnested, but it's going to be within a single original row. So for example, one is going to get paired with 2, 3, and one is going to get paired with 4, 5, but one will never ever be paired with 7, 8. Because 7, 8 came from a different row. And so what it really looks like this, right, is that I'm going to pair one from over here with the 2, 3, and the 4, 5. So two of my output rows are going to be 1, 2, 3, 1, 4, 5. And then the 6 is going to get paired with both of these as well, right? So 6 is going to become 6, 7, 8, and 6, 9, 10, right? It's going to get every combination. It's never going to pair across two of the original rows.
================================================================================

[RANK #12] Chunk 20 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-11_05_25-09%3A55%3A10/1_qjba2w7v
Timestamp: 00:34:20,850
Token count: 431
--------------------------------------------------------------------------------
And because I could look things up like that, I could maybe have a key, which would be the row and the column, and that would give me back a value, right? And it's kind of strange, right? But if I have some row and I just want to add some new value, I can just do it. and I can add whatever column name I want, it doesn't matter if I'm the only row in the world that's using that column name, I can just do it, and it's going to be efficient, and this is a normal way for things to behave, right? So it's kind of very strange, right? It's maybe harder to take a problem like in the real world and see how it maps to this than it is for like a traditional database, but maybe I'll give you one example. So many years ago, I was an intern at, you know, it was Facebook at the time, and I was studying how they were storing Facebook messages, and they were storing them inside of HBase. And basically what they would do is they would say every Facebook user would get one row of data in HBase, and all of your data has to go in that row, right? If you've sent a thousand messages, every message you've ever sent goes in your row, right? And so how can we do that? As you keep getting more messages for a user, what do you do? You just keep adding columns, right? Maybe I have a new message and I'll give it some column ID that's brand new and I'll throw that message in that new column for that row. So in some ways how this actually feels is that I have a collection of dictionaries, right? I have a dictionary for each user and within that dictionary I can have whatever keys I want, right? They call it a table but it kind of feels more like, hey, it's just a collection of these dictionaries, right? They're just trying to keep growing over time.
================================================================================

[RANK #13] Chunk 21 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-10_27_25-09%3A54%3A06/1_bvt1onkv
Timestamp: 00:36:31,890
Token count: 462
--------------------------------------------------------------------------------
I'm talking about specific machines, specific partitions of data, what actually happens when we run it. So now I see I have two machines, and I'm showing what these partitions are for my calculations. And so what we're going to do is we're going to bring together all of the A's in a single partition, and we can do a calculation there. I guess all the B's are together. But what's really different than the logical view is that some of these partitions might have multiple letters in them, and that's kind of really the common case. I didn't show it in a lot of scenarios here, but very often I'll have multiple letters in the same partition, but the rule is that I can't have the same letter split across multiple partitions. So I'm bringing all the related data together plus some other data too, right? So I can do that, and then I can then draw through, and I could calculate for each of these what the results are, right? And so maybe some of these partitions where there's only one, I would just have one output row. If I end up one of these partitions where there's multiple different groups mixed together, then for those, I'd actually have multiple output rows. And then my final results from this query would be all of those three final partitions. Okay, and we'll be talking more about, well, how do we actually figure out how to bring this relay data together? The thing I want you to point to see that matters for performance here is that sometimes I'm pulling this data around, the data stays on the same machine, and sometimes it moves across machines, right? Like these A's, right, in this case, happen to stay on the same machine. But this A down here, right, ended up here. So I went across a different machine. So I actually highlighted these arrows, right? All these arrows over here that are crossing machine boundaries, those represent network I.O. Those are expensive. If we had a clever way to avoid that, we would want to avoid that cross-machine in communication, because that can be a bottleneck.
================================================================================

[RANK #14] Chunk 9 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-11_05_25-09%3A55%3A10/1_qjba2w7v
Timestamp: 00:14:54,270
Token count: 446
--------------------------------------------------------------------------------
So, you know, I asked, you know, maybe this question and it's a no. So I go left. I ask a question over here. It's yes. I go right. And then I see these are all the rows that respond to those questions the way I just did, right? And so I have some questions like this. I have my rows in different places. Some of these leaf nodes will have lots of rows associated with them. Some of them will have very few nodes associated with them. And the placement of these rows in this picture is based on the questions in the decision tree. And I may call that a logical view of the data because it tells me what the decision tree thinks of the row, but it doesn't tell me where that row actually is in a cluster, right? So for example, these two rows belong to the same node in the decision tree, but they might be on completely different machines in my cluster, right? So this is a logical view that I have, right? Now what we're going to do when we have this tree is that we want to make more split points. Maybe I want to split the rows for G over here. Maybe I want to split them for E over here. And we want to see how we can keep running spark jobs to do this splitting. But as we do that, we don't want to have to move our data around unless our data is small. So that was the logical view on the right. I want to look at what the physical view is on the left. So I have here, I have these kind of big solid boxes here. I have one here and one here, and those are machines. They're actually different computers, and I'm calling them Spark executors, and I also have three Spark partitions, right? So this first machine has one partition, the other machine has two partitions, and the rows are in no particular order. Just trying to have an arbitrary order across these three partitions and two machines. And usually, right, it would probably be like 100 machines, but I'm trying to fit it on the slide.
================================================================================

[RANK #15] Chunk 9 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Michael%20Doescher-Morgridge%201570-10_08_25-09%3A53%3A58/1_vudu9ulk
Timestamp: 00:14:22,930
Token count: 472
--------------------------------------------------------------------------------
Now when you're building an application, you can decide where you want to store your data. And one option is you could store your data directly in a file, right? For example, on the left, I'm storing some data in a file system and maybe I'm storing that in table.csv or some other format. And that file system is of course saving that data in blocks in a block device or maybe another file system. The alternative that we're going to be talking about more in upcoming lectures is that you could save your data in a database, but even then you have to think a little bit about the file system because that database usually doesn't use a block device directly. Usually the database is going to be storing its data in some files in a file system. Like maybe, for example, if I have a SQLite database, it's going to store that usually in a .db file would be the convention. And so I just want you to be aware that, hey, there's these different approaches, and then we're going to be talking more about, well, when you might choose one or the other or whatever, the trade-offs here. All right, so when we talk about formats, there's these four things I want to talk about, orientation, encoding, compression, schemas, and I'm going to compare CSVs to these parquet files because they're different along all of these dimensions. So first off, orientation. CSV is what we call a role-oriented format, and parquet is a column-oriented format. I want to look at what that what that means. We talked last time about this txt file that we wanted to access with all these weather stations around the world and this is not exactly a CSV file but it's kind of similar to that and what we saw last time is that if I think about where this data these bytes of data actually live in blocks this is across three different blocks and if I wanted to if I wanted to read all the stations names I'm not reading all the data but I still have to to read all the blocks in the end, right? Because the data that I'm interested in spans all of the blocks. That'd be kind of slow.
================================================================================

[RANK #16] Chunk 3 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-10_27_25-09%3A54%3A06/1_bvt1onkv
Timestamp: 00:05:40,170
Token count: 190
--------------------------------------------------------------------------------
I'm going to limit to 10 as before and then say two pandas. And then I can go ahead and I can see each of these. I think right now it's pairing them up. I didn't really select from one or the other, right? So it's just trying to select everything and so now the date and the holiday are at the end but um same idea right so when i'm doing this i can either use the data frame api to join things or the spark sql api to join things all right so i want to take this a step further so we are able to pair them up and so i want to see how we can do a join in combination with group by so i want to see how many calls occurred on each type of holiday. And so I will use this up here as a starting point. So I'm going to grab this and I'm going to head down here.
================================================================================

[RANK #17] Chunk 23 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-10_17_25-09%3A54%3A10/1_u6knxi3c
Timestamp: 00:41:08,390
Token count: 298
--------------------------------------------------------------------------------
And this function is going to return true or false. It's going to return true if that row starts with an A. Otherwise, it'll return false. And the idea is that when I do this filter transformation, I may take these rows from double, and I'm only going to get the ones back where it was true. All right. So I could write that code. And then at the very end, I could say dot collect. That means, hey, I actually want to see the results. I don't want a recipe anymore. I want to bake the cake based on this recipe. And the cake, in this case, would be those two rows, A2, A6. I'd get those results back. So I could have these four operations, and I could basically get these results back at the very end, right? So all of these different operations I'm showing, I showed four of them, parallelize, map, filter, collect, they fall into two categories. Some are called actions. Transformation, what it does is it produces an RDD, usually based on a different RDD, and because it's an RDD, it doesn't actually do any work yet. It just describes, you know, it records internally, like here's the computation you would need to do to see this data, but it doesn't actually do the computation yet. It's lazy, right?
================================================================================

[RANK #18] Chunk 23 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-09_26_25-09%3A53%3A29/1_56z0285i
Timestamp: 00:38:12,610
Token count: 402
--------------------------------------------------------------------------------
I'm seeing kind of a mix of answers. I feel like three is the most cache-friendly because if I'm going to loop over all of these strings, my goal is to access the fewest cache lines as possible, right? And if I can have all my data very compact there. They only fit in a few cache lines. If I have something like the metal case, right, which is what, you know, is usually done, right, that happens like everywhere, right, then for each reference I have to go off and I have to get these strings in different places, and if those strings are short, they're probably not using a whole cache line, right, so maybe I have, you know, A is probably like one byte, but I have to read in 64 bytes when I access A, another 64 bytes for D, another 64 bytes for BC. Do people have any questions about why the third one is the most cache-friendly. Now, the third is the most cache-friendly, but there's also some challenges here, right? One of those challenges is how can I have like a null value or a non-value? I use different words in different languages, right? Because when I have a reference, it's easy to have a null value, right? I just have the reference not refer to anything, right? But here, if I have all my strings back to back, it's hard to have a null value. The other thing is, if I have an array of strings, it's really easy to say, give me the string at index 2, right? I find the position of that, and I follow the reference to that string. When I have all my strings inline back-to-back, how do I find the start of the string at index 2, right? Because all these strings are different lengths, right? It can be hard to know where that position is, right?
================================================================================

[RANK #19] Chunk 27 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-12_01_25-09%3A55%3A50/1_928i3yai
Timestamp: 00:46:17,440
Token count: 469
--------------------------------------------------------------------------------
So we have Colossus at the bottom, we have these different Google services on top of it, and I want to think about how our data could exist in different places and and how I might work with it. So I'm gonna imagine, maybe for this example, I'm trying to like look for sightings of animals on beaches, like oh I saw a dolphin at this beach, I saw a shark at that beach, so on and so forth. I'm gonna imagine that I have these parquet files just like uploaded to a Google bucket that are describing these things and I want to run some big query queries on them and so one of the things I could do is I could just create a standard table and the way I would do that as I would load the data maybe in this parquet format over into a separate copy copying the the capacitor format and then big query is optimized that that would be pretty efficient to use that once it's there it's a standard table the other thing I could do is I could create what is called an external table and that means instead of copying I'm just trying to register with big query that this parquet file exists over this bucket and I can see what columns are there and I can query from it but what BigQuery will do is whenever I'm using it is trying to go look at the original bucket instead of copying the data around. It's nice I don't have to copy but but it's probably a little less efficient right because BigQuery was designed around capacitor not around parquet. You can imagine I could have other things that I might want to create there may be a combination of these maybe I want to have a table called beach animals that joins these two data sources together, right? So I could do that. The way I could create that is I could say create a view with that name and then I could have an underlying query that says, okay, if you need that data, here's how you produce that data on demand, right? That's why I'm drawing it with this dashed line, right? There's not an actual copy of the data, it's just describing how you could get that data on demand if you wanted to.
================================================================================

[RANK #20] Chunk 20 from FILLER TITLE
URL: https://mediaspace.wisc.edu/media/Tyler%20Caraza-Harter-Morgridge%201570-09_26_25-09%3A53%3A29/1_56z0285i
Timestamp: 00:33:14,940
Token count: 299
--------------------------------------------------------------------------------
There's all these cases where you want to access everything in a column. Now, that's bad because if everything in column one, you know, is spread across these different rows, right? I get one piece of this row, one piece of this row. And so, you know, I look at that matrix and I can very quickly add up all the numbers in a row, but it would be very slow to add up all the numbers in a column, right? what I wish is that all the numbers in a column were contiguous, but that's not the case. All the numbers in the row are contiguous. And so I want to look at how that actually shows up as code in NumPy, right? So NumPy lets us create these matrices. And on the left, I have this first example. I say NumPy array, and then I have like 1, 2 row, 3, 4 row, 5, 6 row. All these rows are contiguous. And then over here, I'm doing something else that's kind of tricky, right? I'm saying .t. Any NumPy experts here? What does .t do? Transpose. Yeah, the rows become columns. The columns become rows. And when I transpose something, there's different ways you could imagine they could do it. Kind of one way is that they could copy all the data somewhere else and rearrange it.
================================================================================

